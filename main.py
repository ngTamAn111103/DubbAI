import subprocess
import sys
import os
import torch
import whisper
from transformers import pipeline 
from pydub import AudioSegment
import json
import pprint
import argparse


# ---- C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n ----
# L·∫•y ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi c·ªßa t·ªáp script n√†y (B√äN TRONG container)
# v√≠ d·ª•: /app/main.py
script_path = os.path.abspath(__file__)

# L·∫•y ƒë∆∞·ªùng d·∫´n th∆∞ m·ª•c cha ch·ª©a t·ªáp script
# BASE_DIR s·∫Ω l√†: /app
BASE_DIR = os.path.dirname(script_path)

# X√¢y d·ª±ng c√°c ƒë∆∞·ªùng d·∫´n kh√°c d·ª±a tr√™n BASE_DIR
SOURCE_FOLDER = os.path.join(BASE_DIR, "source")
VIDEO_INPUT_NAME = "test1.mp4"
AUDIO_OUTPUT_NAME = "original_audio.wav"

VIDEO_INPUT_PATH = os.path.join(SOURCE_FOLDER, VIDEO_INPUT_NAME)
AUDIO_OUTPUT_PATH = os.path.join(SOURCE_FOLDER, AUDIO_OUTPUT_NAME)

# T·ªáp JSON ch·ª©a k·∫øt qu·∫£ phi√™n √¢m
TRANSCRIPT_OUTPUT_NAME = "original_transcript.json"
TRANSCRIPT_OUTPUT_PATH = os.path.join(SOURCE_FOLDER, TRANSCRIPT_OUTPUT_NAME)
# T·ªáp JSON d·ªãch thu·∫≠t Anh -> Vi·ªát
TRANSLATED_TRANSCRIPT_NAME = "translated_transcript.json"
TRANSLATED_TRANSCRIPT_PATH = os.path.join(SOURCE_FOLDER, TRANSLATED_TRANSCRIPT_NAME)
# T·ªáp ch·ª©a m·∫£ng d·ªØ li·ªáu TTS
# Ch√∫ng ta c√≥ th·ªÉ l∆∞u n√≥ d∆∞·ªõi d·∫°ng t·ªáp .py ƒë·ªÉ d·ªÖ import sau n√†y
TTS_DATA_NAME = "tts_data.py" 
TTS_DATA_PATH = os.path.join(SOURCE_FOLDER, TTS_DATA_NAME)
# T·ªáp ƒë·∫ßu ra cho B∆∞·ªõc 6
FINAL_AUDIO_NAME = "dubbed_audio.wav"
FINAL_AUDIO_PATH = os.path.join(SOURCE_FOLDER, FINAL_AUDIO_NAME)
FINAL_VIDEO_NAME = "final_dubbed_video.mp4"
FINAL_VIDEO_PATH = os.path.join(SOURCE_FOLDER, FINAL_VIDEO_NAME)

# C·∫•u h√¨nh m√¥ h√¨nh
WHISPER_MODEL_NAME = "medium.en"
# M√¥ h√¨nh d·ªãch thu·∫≠t
TRANSLATION_MODEL_NAME = "Helsinki-NLP/opus-mt-en-vi"

# C·∫•u h√¨nh t√πy ch·ªçn cho Whisper
# ƒê√¢y l√† n∆°i b·∫°n "tinh ch·ªânh" (tune) ƒë·ªÉ s·ª≠a l·ªói m·ªëc th·ªùi gian
WHISPER_OPTIONS = {
    "no_speech_threshold": 0.3,  # H·∫° th·∫•p ng∆∞·ª°ng ƒë·ªÉ d·ªÖ ph√°t hi·ªán im l·∫∑ng h∆°n (M·∫∑c ƒë·ªãnh 0.6)
    "hallucination_silence_threshold": 3.0, # X√≥a ·∫£o gi√°c trong kho·∫£ng l·∫∑ng > 3 gi√¢y
    "word_timestamps": True,     # B·∫≠t ƒë·ªÉ tƒÉng ƒë·ªô ch√≠nh x√°c c·ªßa m·ªëc th·ªùi gian
    "fp16": False                # ƒê·∫∑t l√† False n·∫øu ch·∫°y tr√™n CPU (an to√†n)
}
# ----------------------------------------

def get_device() -> str:
    """Ki·ªÉm tra v√† tr·∫£ v·ªÅ thi·∫øt b·ªã (device) ph√π h·ª£p cho PyTorch."""
    if torch.cuda.is_available():
        print("Ph√°t hi·ªán GPU CUDA. ƒêang s·ª≠ d·ª•ng 'cuda'.")
        return "cuda"
    elif torch.backends.mps.is_available():
        print("Ph√°t hi·ªán Apple Silicon (M-series). ƒêang s·ª≠ d·ª•ng 'mps'.")
        return "mps"
    else:
        print("Kh√¥ng ph√°t hi·ªán GPU/MPS. ƒêang s·ª≠ d·ª•ng 'cpu'.")
        return "cpu"
    
def extract_audio(video_input_path: str, audio_output_path: str) -> str | None:
    """
    S·ª≠ d·ª•ng ffmpeg ƒë·ªÉ t√°ch √¢m thanh t·ª´ t·ªáp video ƒë·∫ßu v√†o.
    
    Ch√∫ng ta s·∫Ω chuy·ªÉn ƒë·ªïi √¢m thanh th√†nh ƒë·ªãnh d·∫°ng WAV, 16kHz, mono.
    ƒê√¢y l√† ƒë·ªãnh d·∫°ng t·ªëi ∆∞u cho c√°c m√¥ h√¨nh AI Speech-to-Text nh∆∞ Whisper.
    
    Args:
        video_input_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn t·ªáp video .mp4 (v√≠ d·ª•: /app/source/input_video.mp4)
        audio_output_path: ƒê∆∞·ªùng d·∫´n l∆∞u t·ªáp √¢m thanh .wav (v√≠ d·ª•: /app/source/original_audio.wav)

    Returns:
        Tr·∫£ v·ªÅ ƒë∆∞·ªùng d·∫´n t·ªáp √¢m thanh n·∫øu th√†nh c√¥ng, ng∆∞·ª£c l·∫°i tr·∫£ v·ªÅ None.
    """
    # print(f"B·∫Øt ƒë·∫ßu B∆∞·ªõc 1: T√°ch √¢m thanh t·ª´ '{video_input_path}'...")
    
    # X√¢y d·ª±ng l·ªánh ffmpeg
    # -i : T·ªáp ƒë·∫ßu v√†o
    # -vn : B·ªè qua video (no video)
    # -acodec pcm_s16le : ƒê·ªãnh d·∫°ng √¢m thanh l√† WAV 16-bit
    # -ar 16000 : T·∫ßn s·ªë l·∫•y m·∫´u 16kHz (t·ªët nh·∫•t cho Whisper)
    # -ac 1 : 1 k√™nh √¢m thanh (mono)
    # -y : T·ª± ƒë·ªông ghi ƒë√® t·ªáp ƒë·∫ßu ra n·∫øu ƒë√£ t·ªìn t·∫°i
    command = [
        'ffmpeg',
        '-i', video_input_path,
        '-vn',
        '-acodec', 'pcm_s16le',
        '-ar', '16000',
        '-ac', '1',
        '-y',
        audio_output_path
    ]
    
    try:
        # Ch·∫°y l·ªánh
        # capture_output=True: L·∫•y stdout v√† stderr
        # text=True: Gi·∫£i m√£ stdout/stderr th√†nh text (thay v√¨ bytes)
        # check=True: T·ª± ƒë·ªông n√©m l·ªói (raise Exception) n·∫øu ffmpeg tr·∫£ v·ªÅ m√£ l·ªói
        subprocess.run(command, check=True, capture_output=True, text=True)
        
        print(f"‚úÖ B∆∞·ªõc 1 v√† 2 ƒë√£ ho√†n th√†nh! √Çm thanh ƒë√£ ƒë∆∞·ª£c t√°ch v√† l∆∞u t·∫°i:")
        print(f"   {audio_output_path}")
        return audio_output_path
        
    except FileNotFoundError:
        print("‚ùå L·ªñI: Kh√¥ng t√¨m th·∫•y 'ffmpeg'. H√£y ƒë·∫£m b·∫£o n√≥ ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t trong Dockerfile.")
        return None
    except subprocess.CalledProcessError as e:
        # N·∫øu ffmpeg ch·∫°y b·ªã l·ªói (v√≠ d·ª•: kh√¥ng t√¨m th·∫•y file input)
        print(f"‚ùå L·ªñI: ffmpeg th·∫•t b·∫°i v·ªõi m√£ l·ªói {e.returncode}")
        print("   L·ªói chi ti·∫øt (stderr):")
        print(f"   {e.stderr}")
        return None
    except Exception as e:
        print(f"‚ùå L·ªñI kh√¥ng x√°c ƒë·ªãnh: {e}")
        return None

def transcribe_audio(audio_path: str, model_name: str, device: str) -> list[dict] | None:
    """
    S·ª≠ d·ª•ng Whisper ƒë·ªÉ phi√™n √¢m √¢m thanh v√† l·∫•y m·ªëc th·ªùi gian.
    
    Args:
        audio_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn t·ªáp √¢m thanh .wav
        model_name: T√™n m√¥ h√¨nh Whisper (v√≠ d·ª•: "base", "small", "medium")

    Returns:
        M·ªôt danh s√°ch (list) c√°c 'segments'. 
        V√≠ d·ª•: [
            {'start': 0.0, 'end': 5.2, 'text': ' Hello world.'},
            {'start': 5.2, 'end': 8.0, 'text': ' This is a test.'}
        ]
        Tr·∫£ v·ªÅ None n·∫øu c√≥ l·ªói.
    """

    try:
        model = whisper.load_model(model_name, device=device)
        
        # C·∫≠p nh·∫≠t t√πy ch·ªçn fp16 d·ª±a tr√™n thi·∫øt b·ªã
        transcribe_options = WHISPER_OPTIONS.copy()
        transcribe_options["fp16"] = (device != "cpu")

        # S·ª≠ d·ª•ng **ƒë·ªÉ gi·∫£i n√©n (unpack) dictionary v√†o c√°c tham s·ªë
        result = model.transcribe(audio_path, task="transcribe", **transcribe_options)
        
        print(f"‚úÖ B∆∞·ªõc 3 ho√†n th√†nh! Ng√¥n ng·ªØ: {result.get('language', 'kh√¥ng r√µ')}")
        
        # In ra segment ƒë·∫ßu ti√™n ƒë·ªÉ ki·ªÉm tra m·ªëc th·ªùi gian
        if result['segments']:
            seg0 = result['segments'][0]
            print(f"   Ki·ªÉm tra: Segment 0 b·∫Øt ƒë·∫ßu t·ª´ {seg0['start']:.2f}s")
        
        return result['segments']
    except Exception as e:
        print(f"‚ùå L·ªñI trong qu√° tr√¨nh phi√™n √¢m: {e}")
        return None
    
def translate_segments(segments: list[dict], model_name: str, device: str) -> list[dict] | None:
    """
    D·ªãch vƒÉn b·∫£n trong c√°c segments sang ti·∫øng Vi·ªát.
    
    Args:
        segments: Danh s√°ch segments t·ª´ Whisper (ch·ª©a 'start', 'end', 'text').
        model_name: T√™n m√¥ h√¨nh d·ªãch tr√™n Hugging Face.
        device: Thi·∫øt b·ªã ƒë·ªÉ ch·∫°y (cpu, cuda, mps).

    Returns:
        Danh s√°ch segments m·ªõi v·ªõi 'text' ƒë√£ ƒë∆∞·ª£c d·ªãch.
    """
    
    try:
        
        # PyTorch index cho thi·∫øt b·ªã (0 cho cuda/mps, -1 cho cpu)
        torch_device_index = 0 if device in ["cuda", "mps"] else -1
        translator = pipeline("translation", 
                              model=model_name, 
                              device=torch_device_index)

        # 2. Chu·∫©n b·ªã d·ªØ li·ªáu (d·ªãch theo batch cho nhanh)
        # L·∫•y vƒÉn b·∫£n (ƒë√£ lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a) t·ª´ m·ªói segment
        texts_to_translate = [segment['text'].strip() for segment in segments]
        
        # 3. Th·ª±c hi·ªán d·ªãch
        translated_results = translator(texts_to_translate, batch_size=16) # batch_size=16

        # 4. T·∫°o l·∫°i danh s√°ch segments v·ªõi vƒÉn b·∫£n ƒë√£ d·ªãch
        translated_segments = []
        for i, segment in enumerate(segments):
            translated_text = translated_results[i]['translation_text']
            
            new_segment = {
                "id": segment['id'],
                "start": segment['start'],
                "end": segment['end'],
                "original_text": segment['text'], # Gi·ªØ l·∫°i vƒÉn b·∫£n g·ªëc ƒë·ªÉ tham kh·∫£o
                "text": translated_text  # Thay th·∫ø b·∫±ng vƒÉn b·∫£n ƒë√£ d·ªãch
            }
            translated_segments.append(new_segment)
            
        # print(f"‚úÖ B∆∞·ªõc 4 ho√†n th√†nh!")
        return translated_segments
        
    except Exception as e:
        print(f"‚ùå L·ªñI trong qu√° tr√¨nh d·ªãch thu·∫≠t: {e}")
        return None
    
def generate_tts_data_file(translated_segments: list[dict], output_script_path: str):
    """
    T·∫°o m·∫£ng d·ªØ li·ªáu TTS v√† GHI N·ªòI DUNG M·∫¢NG ƒë√≥ ra t·ªáp.
    ƒê·ªìng th·ªùi tr·∫£ v·ªÅ danh s√°ch segments ƒë√£ c·∫≠p nh·∫≠t cho B∆∞·ªõc 6.
    """
    # print(f"\nB·∫Øt ƒë·∫ßu B∆∞·ªõc 5: Ghi m·∫£ng d·ªØ li·ªáu TTS v√†o '{output_script_path}'...")
    
    tts_data_list = []
    segments_with_audio_path = []
    
    try:
        # L·∫∑p qua c√°c segment ƒë√£ d·ªãch
        for segment in translated_segments:
            segment_id = segment['id']
            text_to_speak = segment['text'].strip()
            
            # 1. T·∫°o d·ªØ li·ªáu cho m·∫£ng
            formatted_text = f"[KienThucQuanSu]{text_to_speak}"
            audio_output_file = f"audio_VN/{segment_id}.wav"
            
            tts_tuple = (formatted_text, audio_output_file)
            tts_data_list.append(tts_tuple)
            
            # 2. C·∫≠p nh·∫≠t segment cho B∆∞·ªõc 6
            segment['audio_path'] = os.path.join(SOURCE_FOLDER, audio_output_file)
            segments_with_audio_path.append(segment)

        # 3. Ghi m·∫£ng (d∆∞·ªõi d·∫°ng chu·ªói) ra t·ªáp
        with open(output_script_path, 'w', encoding='utf-8') as f:
            # S·ª≠ d·ª•ng pprint.pformat ƒë·ªÉ t·∫°o chu·ªói Python ƒë·∫πp
            # indent=4 v√† width=120 (ƒë·ªÉ tr√°nh ng·∫Øt d√≤ng qu√° s·ªõm)
            # S·∫Ω t·∫°o ra ƒë·ªãnh d·∫°ng gi·ªëng h·ªát v√≠ d·ª• c·ªßa b·∫°n
            file_content = pprint.pformat(tts_data_list, indent=4, width=120)
            
            # Ghi v√†o t·ªáp. 
            # (B·∫°n c√≥ th·ªÉ th√™m `tts_data = ` ·ªü ƒë·∫ßu n·∫øu mu·ªën n√≥ l√† t·ªáp .py)
            f.write("tts_data = ")
            f.write(file_content)
            f.write("\n") 

        print(f"‚úÖ B∆∞·ªõc 5 ho√†n th√†nh! ƒê√£ ghi m·∫£ng d·ªØ li·ªáu v√†o t·ªáp.")
        # Tr·∫£ v·ªÅ danh s√°ch segment ƒë√£ c·∫≠p nh·∫≠t cho B∆∞·ªõc 6
        return segments_with_audio_path

    except Exception as e:
        print(f"‚ùå L·ªñI trong qu√° tr√¨nh ghi t·ªáp d·ªØ li·ªáu TTS: {e}")
        return None
    
def apply_ffmpeg_atempo(input_segment: AudioSegment, speed: float, 
                        temp_dir: str = "/tmp") -> AudioSegment:
    """
    S·ª≠ d·ª•ng ffmpeg v·ªõi b·ªô l·ªçc 'atempo' ƒë·ªÉ co/d√£n √¢m thanh m·ªôt c√°ch an to√†n.
    H√†m n√†y x·ª≠ l√Ω c√°c gi·ªõi h·∫°n 0.5-100.0 c·ªßa atempo.
    """
    if abs(speed - 1.0) < 0.01:
        return input_segment # Kh√¥ng c·∫ßn thay ƒë·ªïi

    # T·∫°o ƒë∆∞·ªùng d·∫´n t·ªáp t·∫°m
    # Ch√∫ng ta ph·∫£i l∆∞u segment ra t·ªáp ƒë·ªÉ ffmpeg ƒë·ªçc
    temp_input = os.path.join(temp_dir, "temp_atempo_in.wav")
    temp_output = os.path.join(temp_dir, "temp_atempo_out.wav")
    
    input_segment.export(temp_input, format="wav")

    # X√¢y d·ª±ng chu·ªói b·ªô l·ªçc atempo
    # V√≠ d·ª•: speed = 0.3 -> [0.6, 0.5] (v√¨ 0.5 * 0.6 = 0.3)
    # V√≠ d·ª•: speed = 0.2 -> [0.8, 0.5, 0.5] (v√¨ 0.5 * 0.5 * 0.8 = 0.2)
    filters = []
    current_speed = speed
    
    # X·ª≠ l√Ω t·ªëc ƒë·ªô qu√° th·∫•p (< 0.5)
    while current_speed < 0.5:
        filters.append("atempo=0.5")
        current_speed /= 0.5 # T·ªëc ƒë·ªô c√≤n l·∫°i ƒë·ªÉ √°p d·ª•ng
    
    # X·ª≠ l√Ω t·ªëc ƒë·ªô qu√° cao (> 100.0)
    while current_speed > 100.0:
        filters.append("atempo=100.0")
        current_speed /= 100.0

    # √Åp d·ª•ng ph·∫ßn t·ªëc ƒë·ªô c√≤n l·∫°i (v√≠ d·ª•: 0.6, ho·∫∑c 1.5, ho·∫∑c 0.8)
    if abs(current_speed - 1.0) > 0.01:
        filters.append(f"atempo={current_speed}")

    # N·ªëi c√°c b·ªô l·ªçc l·∫°i, v√≠ d·ª•: "atempo=0.8,atempo=0.5,atempo=0.5"
    filter_chain = ",".join(filters)

    # X√¢y d·ª±ng v√† ch·∫°y l·ªánh ffmpeg
    command = [
        'ffmpeg',
        '-i', temp_input,
        '-filter:a', filter_chain,
        '-y', temp_output
    ]
    
    try:
        subprocess.run(command, check=True, capture_output=True, text=True)
        # T·∫£i t·ªáp k·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c co/d√£n
        output_segment = AudioSegment.from_wav(temp_output)
        
        # D·ªçn d·∫πp t·ªáp t·∫°m
        os.remove(temp_input)
        os.remove(temp_output)
        
        return output_segment
        
    except Exception as e:
        print(f"   ‚ùå L·ªñI khi ƒëang ch·∫°y atempo (t·ªëc ƒë·ªô {speed:.2f}x): {e}")
        print(f"   ...S·ª≠ d·ª•ng segment g·ªëc (kh√¥ng ƒë·ªìng b·ªô) thay th·∫ø.")
        # D·ªçn d·∫πp t·ªáp t·∫°m
        if os.path.exists(temp_input): os.remove(temp_input)
        if os.path.exists(temp_output): os.remove(temp_output)
        return input_segment # Tr·∫£ v·ªÅ b·∫£n g·ªëc n·∫øu th·∫•t b·∫°i
    
    
def synchronize_and_combine(segments_with_audio_path: list[dict], 
                            final_audio_path: str) -> str | None:
    """
    (B∆∞·ªõc 6.1) ƒê·ªìng b·ªô (co/d√£n) c√°c t·ªáp TTS v√† n·ªëi ch√∫ng l·∫°i.
    Phi√™n b·∫£n n√†y s·ª≠ d·ª•ng ffmpeg atempo thay v√¨ pydub.speedup.
    """
    print(f"\nB·∫Øt ƒë·∫ßu B∆∞·ªõc 6.1: ƒê·ªìng b·ªô v√† N·ªëi c√°c t·ªáp √¢m thanh...")
    
    final_audio = AudioSegment.empty()
    last_segment_end_ms = 0.0 # Theo d√µi m·ªëc th·ªùi gian cu·ªëi c√πng (t√≠nh b·∫±ng ms)
    
    try:
        # L·∫∑p qua c√°c segment ƒë√£ c√≥ ƒë∆∞·ªùng d·∫´n 'audio_path'
        for i, segment in enumerate(segments_with_audio_path):
            
            print(f"--- ƒêang x·ª≠ l√Ω segment {i} (ID: {segment['id']}) ---")
            
            target_start_ms = segment['start'] * 1000
            target_end_ms = segment['end'] * 1000
            target_duration_ms = target_end_ms - target_start_ms

            # 1. X·ª≠ l√Ω kho·∫£ng l·∫∑ng (Silence)
            if target_start_ms > last_segment_end_ms:
                silence_duration = target_start_ms - last_segment_end_ms
                final_audio += AudioSegment.silent(duration=silence_duration)
                print(f"   ... Th√™m {silence_duration:.0f}ms kho·∫£ng l·∫∑ng.")
                
            # 2. T·∫£i t·ªáp √¢m thanh TTS
            audio_file_path = segment['audio_path']
            if not os.path.exists(audio_file_path):
                print(f"   ‚ö†Ô∏è C·∫¢NH B√ÅO: Kh√¥ng t√¨m th·∫•y t·ªáp {audio_file_path}. B·ªè qua segment.")
                last_segment_end_ms = target_end_ms
                continue

            tts_segment = AudioSegment.from_wav(audio_file_path)
            current_duration_ms = len(tts_segment)
            
            # 3. ƒê·ªìng b·ªô th·ªùi gian (Time-Stretching)
            if target_duration_ms <= 0 or current_duration_ms <= 0:
                print(f"   ‚ö†Ô∏è C·∫¢NH B√ÅO: Segment {i} c√≥ th·ªùi l∆∞·ª£ng kh√¥ng h·ª£p l·ªá. B·ªè qua.")
                last_segment_end_ms = target_end_ms
                continue
            
            playback_speed = current_duration_ms / target_duration_ms

            print(f"   ƒê·ªìng b·ªô segment {i}: {current_duration_ms:.0f}ms -> {target_duration_ms:.0f}ms (t·ªëc ƒë·ªô {playback_speed:.2f}x)")

            # === KH·ªêI LOGIC M·ªöI (V3.0) ===
            # G·ªçi h√†m helper ffmpeg atempo c·ªßa ch√∫ng ta
            processed_segment = apply_ffmpeg_atempo(tts_segment, playback_speed)
            # === K·∫æT TH√öC KH·ªêI LOGIC M·ªöI ===

            # 4. N·ªëi √¢m thanh ƒë√£ x·ª≠ l√Ω
            final_audio += processed_segment
            last_segment_end_ms = target_end_ms

        # 5. L∆∞u t·ªáp √¢m thanh cu·ªëi c√πng
        print(f"ƒêang l∆∞u t·ªáp √¢m thanh l·ªìng ti·∫øng cu·ªëi c√πng t·∫°i: {final_audio_path}")
        final_audio.export(final_audio_path, format="wav")
        print(f"‚úÖ B∆∞·ªõc 6.1 ho√†n th√†nh!")
        return final_audio_path

    except Exception as e:
        print(f"‚ùå L·ªñI trong qu√° tr√¨nh ƒë·ªìng b·ªô √¢m thanh: {e}")
        return None
    
    
def merge_audio_to_video(video_input_path: str, audio_input_path: str, 
                         video_output_path: str) -> str | None:
    """
    Gh√©p t·ªáp √¢m thanh l·ªìng ti·∫øng v√†o video g·ªëc (ƒë√£ x√≥a ti·∫øng).
    """
    
    # L·ªánh ffmpeg
    # -i [video_input]: Video g·ªëc
    # -i [audio_input]: √Çm thanh l·ªìng ti·∫øng m·ªõi
    # -c:v copy: Sao ch√©p lu·ªìng video, kh√¥ng encode l·∫°i (R·∫§T NHANH)
    # -map 0:v:0: Ch·ªçn lu·ªìng video t·ª´ file ƒë·∫ßu v√†o (0)
    # -map 1:a:0: Ch·ªçn lu·ªìng audio t·ª´ file th·ª© hai (1) -> B·ªé √ÇM THANH G·ªêC
    # -shortest: K·∫øt th√∫c video khi lu·ªìng ng·∫Øn nh·∫•t (video ho·∫∑c audio) k·∫øt th√∫c
    # -y: Ghi ƒë√® file ƒë·∫ßu ra
    command = [
        'ffmpeg',
        '-i', video_input_path,
        '-i', audio_input_path,
        '-c:v', 'copy',
        '-map', '0:v:0',
        '-map', '1:a:0',
        '-shortest',
        '-y',
        video_output_path
    ]

    try:
        subprocess.run(command, check=True, capture_output=True, text=True)
        print(f"‚úÖ B∆∞·ªõc 6.2 ho√†n th√†nh! Video l·ªìng ti·∫øng ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i:")
        print(f"   {video_output_path}")
        return video_output_path
    except subprocess.CalledProcessError as e:
        print(f"‚ùå L·ªñI: ffmpeg th·∫•t b·∫°i khi gh√©p video: {e.stderr}")
        return None
    except Exception as e:
        print(f"‚ùå L·ªñI kh√¥ng x√°c ƒë·ªãnh khi gh√©p video: {e}")
        return None
    

# def main():
#     # X√°c ƒë·ªãnh thi·∫øt b·ªã ch·∫°y AI (ch·∫°y 1 l·∫ßn ·ªü ƒë·∫ßu)
#     device = get_device()

#     # Ki·ªÉm tra xem t·ªáp video ƒë·∫ßu v√†o c√≥ t·ªìn t·∫°i kh√¥ng
#     if not os.path.exists(VIDEO_INPUT_PATH):
#         print(f"‚ùå L·ªñI: Kh√¥ng t√¨m th·∫•y t·ªáp video ƒë·∫ßu v√†o t·∫°i:")
#         print(f"   {VIDEO_INPUT_PATH}")
#         print("   H√£y ƒë·∫£m b·∫£o b·∫°n ƒë√£ ƒë·∫∑t video v√†o th∆∞ m·ª•c 'source' v√† ƒë·∫∑t t√™n l√† 'input_video.mp4'")
#         sys.exit(1) # Tho√°t ch∆∞∆°ng tr√¨nh v·ªõi m√£ l·ªói
        
#     # B∆∞·ªõc 1 + 2: T√°ch √¢m thanh
#     extracted_audio_file = extract_audio(VIDEO_INPUT_PATH, AUDIO_OUTPUT_PATH)
    
#     if extracted_audio_file is None:
#         print("D·ª´ng ch∆∞∆°ng tr√¨nh do l·ªói ·ªü B∆∞·ªõc 1.")
#         sys.exit(1)
        
#     # B∆∞·ªõc 3: Phi√™n √¢m (Audio to Text)
#     if os.path.exists(TRANSCRIPT_OUTPUT_PATH):
#         print(f"\nƒê√£ t√¨m th·∫•y t·ªáp phi√™n √¢m: {TRANSCRIPT_OUTPUT_PATH}. B·ªè qua B∆∞·ªõc 3.")
#         with open(TRANSCRIPT_OUTPUT_PATH, 'r', encoding='utf-8') as f:
#             segments = json.load(f)
#     else:
#         segments = transcribe_audio(AUDIO_OUTPUT_PATH, WHISPER_MODEL_NAME, device)
#         if segments is None: sys.exit(1)
        
#         # L∆∞u t·ªáp JSON
#         print(f"\nƒêang l∆∞u k·∫øt qu·∫£ phi√™n √¢m v√†o '{TRANSCRIPT_OUTPUT_PATH}'...")
#         try:
#             with open(TRANSCRIPT_OUTPUT_PATH, 'w', encoding='utf-8') as f:
#                 json.dump(segments, f, indent=4, ensure_ascii=False)
#             print("‚úÖ ƒê√£ l∆∞u phi√™n √¢m th√†nh c√¥ng.")
#         except Exception as e:
#             print(f"‚ùå L·ªñI: Kh√¥ng th·ªÉ l∆∞u t·ªáp JSON phi√™n √¢m: {e}")
#             sys.exit(1)


#     # In ra 3 segment ƒë·∫ßu ti√™n ƒë·ªÉ ki·ªÉm tra
#     print("\n--- K·∫øt qu·∫£ phi√™n √¢m (3 segment ƒë·∫ßu ti√™n) ---")
#     for i, segment in enumerate(segments[:3]):
#         start = segment['start']
#         end = segment['end']
#         text = segment['text'].strip()
#         print(f"[{start:.2f}s -> {end:.2f}s] {text}")
#     print("---------------------------------------------")

#     # B∆∞·ªõc 4: D·ªãch thu·∫≠t
#     translated_segments = translate_segments(segments, TRANSLATION_MODEL_NAME, device)
#     if translated_segments is None: sys.exit(1)
        
#     # L∆∞u t·ªáp JSON ƒë√£ d·ªãch
#     # print(f"\nƒêang l∆∞u k·∫øt qu·∫£ d·ªãch thu·∫≠t v√†o '{TRANSLATED_TRANSCRIPT_PATH}'...")
#     try:
#         with open(TRANSLATED_TRANSCRIPT_PATH, 'w', encoding='utf-8') as f:
#             # ensure_ascii=False R·∫§T QUAN TR·ªåNG ƒë·ªÉ l∆∞u ti·∫øng Vi·ªát
#             json.dump(translated_segments, f, indent=4, ensure_ascii=False)
#         # print("‚úÖ ƒê√£ l∆∞u d·ªãch thu·∫≠t th√†nh c√¥ng.")
#     except Exception as e:
#         print(f"‚ùå L·ªñI: Kh√¥ng th·ªÉ l∆∞u t·ªáp JSON d·ªãch thu·∫≠t: {e}")
#         sys.exit(1)

#     # In ra 3 segment ƒë√£ d·ªãch ƒë·∫ßu ti√™n ƒë·ªÉ ki·ªÉm tra
#     print("\n--- K·∫øt qu·∫£ d·ªãch thu·∫≠t (3 segment ƒë·∫ßu ti√™n) ---")
#     for i, segment in enumerate(translated_segments[:3]):
#         start = segment['start']
#         end = segment['end']
#         text = segment['text'].strip()
#         print(f"[{start:.2f}s -> {end:.2f}s] {text}")
#     print("-------------------------------------------------")
    
#     # B∆∞·ªõc 5: Chu·∫©n b·ªã data cho Colab ch·∫°y
#     if not os.path.exists(TTS_DATA_PATH):
#         segments_with_audio_path = generate_tts_data_file(translated_segments, TTS_DATA_PATH)
#         if segments_with_audio_path is None: sys.exit(1)
        
#         # C·∫≠p nh·∫≠t l·∫°i t·ªáp JSON v·ªõi 'audio_path'
#         try:
#             with open(TRANSLATED_TRANSCRIPT_PATH, 'w', encoding='utf-8') as f:
#                 json.dump(segments_with_audio_path, f, indent=4, ensure_ascii=False)
#             print(f"‚úÖ ƒê√£ c·∫≠p nh·∫≠t t·ªáp d·ªãch thu·∫≠t v·ªõi ƒë∆∞·ªùng d·∫´n √¢m thanh.")
#             translated_segments = segments_with_audio_path # ƒê·∫£m b·∫£o ch√∫ng ta c√≥ b·∫£n m·ªõi nh·∫•t
#         except Exception as e:
#             print(f"‚ùå L·ªñI: Kh√¥ng th·ªÉ c·∫≠p nh·∫≠t t·ªáp JSON d·ªãch thu·∫≠t: {e}")
#     else:
#         print(f"\nƒê√£ t√¨m th·∫•y t·ªáp d·ªØ li·ªáu TTS: {TTS_DATA_PATH}. B·ªè qua B∆∞·ªõc 5.")
#         # ƒê·∫£m b·∫£o `translated_segments` ƒë√£ c√≥ 'audio_path'
#         if 'audio_path' not in translated_segments[0]:
#             print("   C·∫≠p nh·∫≠t l·∫°i ƒë∆∞·ªùng d·∫´n audio cho B∆∞·ªõc 6...")
#             for segment in translated_segments:
#                  segment_id = segment['id']
#                  segment['audio_path'] = os.path.join(SOURCE_FOLDER, f"audio_VN/{segment_id}.wav")

#     # B∆∞·ªõc 6.1: ƒê·ªìng b·ªô v√† N·ªëi √¢m thanh
#     final_audio_file = synchronize_and_combine(translated_segments, FINAL_AUDIO_PATH)
#     if final_audio_file is None:
#         print("D·ª´ng ch∆∞∆°ng tr√¨nh do l·ªói ·ªü B∆∞·ªõc 6.1.")
#         sys.exit(1)

#     # B∆∞·ªõc 6.2: Gh√©p √¢m thanh v√†o video
#     final_video_file = merge_audio_to_video(VIDEO_INPUT_PATH, final_audio_file, FINAL_VIDEO_PATH)
#     if final_video_file is None:
#         print("D·ª´ng ch∆∞∆°ng tr√¨nh do l·ªói ·ªü B∆∞·ªõc 6.2.")
#         sys.exit(1)

#     print("\n--- üéâüéâüéâ HO√ÄN TH√ÄNH TO√ÄN B·ªò D·ª∞ √ÅN! üéâüéâüéâ ---")
#     print(f"Video l·ªìng ti·∫øng cu·ªëi c√πng c·ªßa b·∫°n ƒë√£ s·∫µn s√†ng t·∫°i:")
#     print(f"{FINAL_VIDEO_PATH}")
#     print("-------------------------------------------------")

# ---- H√ÄM MAIN (ƒê√É C·∫§U TR√öC L·∫†I) ----
def main(args): # M·ªöI: 'args' ƒë∆∞·ª£c truy·ªÅn v√†o
    device = get_device()

    # ========== CH·∫æ ƒê·ªò 1: CHU·∫®N B·ªä (PREP) ==========
    if args.step == 'prep':
        print("--- Ch·∫°y ch·∫ø ƒë·ªô 'PREP' (B∆∞·ªõc 1-5) ---")
        
        # --- B∆∞·ªõc 1 + 2: T√°ch √¢m thanh ---
        if not os.path.exists(AUDIO_OUTPUT_PATH):
            extracted_audio_file = extract_audio(VIDEO_INPUT_PATH, AUDIO_OUTPUT_PATH)
            if extracted_audio_file is None: sys.exit(1)
        else:
            print(f"ƒê√£ t√¨m th·∫•y √¢m thanh g·ªëc: {AUDIO_OUTPUT_PATH}. B·ªè qua B∆∞·ªõc 1.")
        
        # --- B∆∞·ªõc 3: Phi√™n √¢m ---
        if os.path.exists(TRANSCRIPT_OUTPUT_PATH):
            print(f"\nƒê√£ t√¨m th·∫•y t·ªáp phi√™n √¢m: {TRANSCRIPT_OUTPUT_PATH}. B·ªè qua B∆∞·ªõc 3.")
            with open(TRANSCRIPT_OUTPUT_PATH, 'r', encoding='utf-8') as f:
                segments = json.load(f)
        else:
            print("\nB·∫Øt ƒë·∫ßu B∆∞·ªõc 3: Phi√™n √¢m √¢m thanh g·ªëc th√†nh vƒÉn b·∫£n")
            segments = transcribe_audio(AUDIO_OUTPUT_PATH, WHISPER_MODEL_NAME, device)
            if segments is None: sys.exit(1)
            # ... (l∆∞u t·ªáp json)
            try:
                with open(TRANSCRIPT_OUTPUT_PATH, 'w', encoding='utf-8') as f:
                    json.dump(segments, f, indent=4, ensure_ascii=False)
                print(f"‚úÖ ƒê√£ l∆∞u phi√™n √¢m v√†o: {TRANSCRIPT_OUTPUT_PATH}")
            except Exception as e:
                print(f"‚ùå L·ªñI: Kh√¥ng th·ªÉ l∆∞u t·ªáp JSON phi√™n √¢m: {e}")
                sys.exit(1)

        # --- B∆∞·ªõc 4: D·ªãch thu·∫≠t ---
        if os.path.exists(TRANSLATED_TRANSCRIPT_PATH):
            print(f"\nƒê√£ t√¨m th·∫•y t·ªáp d·ªãch thu·∫≠t: {TRANSLATED_TRANSCRIPT_PATH}. B·ªè qua B∆∞·ªõc 4.")
            with open(TRANSLATED_TRANSCRIPT_PATH, 'r', encoding='utf-8') as f:
                translated_segments = json.load(f)
        else:
            print("\nB·∫Øt ƒë·∫ßu B∆∞·ªõc 4: D·ªãch thu·∫≠t t·ª´ vƒÉn b·∫£n sang vƒÉn b·∫£n")
            translated_segments = translate_segments(segments, TRANSLATION_MODEL_NAME, device)
            if translated_segments is None: sys.exit(1)
            # ... (l∆∞u t·ªáp json)
            try:
                with open(TRANSLATED_TRANSCRIPT_PATH, 'w', encoding='utf-8') as f:
                    json.dump(translated_segments, f, indent=4, ensure_ascii=False)
                print(f"‚úÖ ƒê√£ l∆∞u d·ªãch thu·∫≠t v√†o: {TRANSLATED_TRANSCRIPT_PATH}")
            except Exception as e:
                print(f"‚ùå L·ªñI: Kh√¥ng th·ªÉ l∆∞u t·ªáp JSON d·ªãch thu·∫≠t: {e}")
                sys.exit(1)

        # --- B∆∞·ªõc 5: Ghi t·ªáp d·ªØ li·ªáu TTS ---
        print("\nB·∫Øt ƒë·∫ßu B∆∞·ªõc 5: Ghi t·ªáp d·ªØ li·ªáu...")
        segments_with_audio_path = generate_tts_data_file(translated_segments, TTS_DATA_PATH)
        if segments_with_audio_path is None: sys.exit(1)

        # C·∫≠p nh·∫≠t l·∫°i t·ªáp JSON v·ªõi ƒë∆∞·ªùng d·∫´n √¢m thanh
        try:
            with open(TRANSLATED_TRANSCRIPT_PATH, 'w', encoding='utf-8') as f:
                json.dump(segments_with_audio_path, f, indent=4, ensure_ascii=False)
            print(f"‚úÖ ƒê√£ c·∫≠p nh·∫≠t t·ªáp d·ªãch thu·∫≠t v·ªõi ƒë∆∞·ªùng d·∫´n √¢m thanh (d·ª± ki·∫øn).")
        except Exception as e:
            print(f"‚ùå L·ªñI: Kh√¥ng th·ªÉ c·∫≠p nh·∫≠t t·ªáp JSON d·ªãch thu·∫≠t: {e}")

        print("\n--- ‚úÖ Ho√†n th√†nh 'PREP' ---")
        print(f"ƒê√£ t·∫°o t·ªáp d·ªØ li·ªáu TTS t·∫°i: {TTS_DATA_PATH}")
        print("B√¢y gi·ªù b·∫°n c√≥ th·ªÉ t·∫°o c√°c t·ªáp .wav trong 'source/audio_VN' tr∆∞·ªõc khi ch·∫°y b∆∞·ªõc 'combine'.")

    # ========== CH·∫æ ƒê·ªò 2: K·∫æT H·ª¢P (COMBINE) ==========
    elif args.step == 'combine':
        print("--- Ch·∫°y ch·∫ø ƒë·ªô 'COMBINE' (B∆∞·ªõc 6) ---")
        
        # --- B∆∞·ªõc 6: ƒê·ªìng b·ªô v√† Gh√©p ---
        # T·∫£i t·ªáp JSON ƒë√£ d·ªãch (ph·∫£i ch·ª©a 'audio_path')
        if not os.path.exists(TRANSLATED_TRANSCRIPT_PATH):
            print(f"‚ùå L·ªñI: Kh√¥ng t√¨m th·∫•y t·ªáp {TRANSLATED_TRANSCRIPT_PATH}.")
            print("B·∫°n ph·∫£i ch·∫°y b∆∞·ªõc 'prep' tr∆∞·ªõc.")
            sys.exit(1)
            
        print(f"ƒêang t·∫£i t·ªáp d·ªãch thu·∫≠t: {TRANSLATED_TRANSCRIPT_PATH}...")
        with open(TRANSLATED_TRANSCRIPT_PATH, 'r', encoding='utf-8') as f:
            translated_segments = json.load(f)

        # Ki·ªÉm tra xem c√°c t·ªáp audio c√≥ th·ª±c s·ª± t·ªìn t·∫°i kh√¥ng
        first_audio_path = translated_segments[0].get('audio_path')
        if first_audio_path is None or not os.path.exists(first_audio_path):
             print(f"‚ùå L·ªñI: Kh√¥ng t√¨m th·∫•y t·ªáp √¢m thanh ƒë·∫ßu ti√™n ({first_audio_path}).")
             print("B·∫°n ƒë√£ t·∫°o c√°c t·ªáp .wav trong 'source/audio_VN' ch∆∞a?")
             sys.exit(1)

        # B∆∞·ªõc 6.1: ƒê·ªìng b·ªô v√† N·ªëi √¢m thanh
        final_audio_file = synchronize_and_combine(translated_segments, FINAL_AUDIO_PATH)
        if final_audio_file is None:
            print("D·ª´ng ch∆∞∆°ng tr√¨nh do l·ªói ·ªü B∆∞·ªõc 6.1.")
            sys.exit(1)

        # B∆∞·ªõc 6.2: Gh√©p √¢m thanh v√†o video
        final_video_file = merge_audio_to_video(VIDEO_INPUT_PATH, final_audio_file, FINAL_VIDEO_PATH)
        if final_video_file is None:
            print("D·ª´ng ch∆∞∆°ng tr√¨nh do l·ªói ·ªü B∆∞·ªõc 6.2.")
            sys.exit(1)

        print("\n--- üéâüéâüéâ HO√ÄN TH√ÄNH TO√ÄN B·ªò D·ª∞ √ÅN! üéâüéâüéâ ---")
        print(f"Video l·ªìng ti·∫øng cu·ªëi c√πng c·ªßa b·∫°n ƒë√£ s·∫µn s√†ng t·∫°i:")
        print(f"{FINAL_VIDEO_PATH}")
        print("-------------------------------------------------")


if __name__ == "__main__":
    # ---- M·ªöI: THI·∫æT L·∫¨P ARGPARSE ----
    parser = argparse.ArgumentParser(description="Quy tr√¨nh l·ªìng ti·∫øng AI.")
    parser.add_argument(
        '--step', 
        type=str, 
        choices=['prep', 'combine'], 
        required=True, 
        help="Ch·ªçn b∆∞·ªõc ƒë·ªÉ ch·∫°y: 'prep' (B∆∞·ªõc 1-5) ho·∫∑c 'combine' (B∆∞·ªõc 6)"
    )
    args = parser.parse_args()
    
    main(args) # Ch·∫°y h√†m main v·ªõi c√°c ƒë·ªëi s·ªë




