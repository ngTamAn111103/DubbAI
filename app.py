import subprocess
import sys
import os
import torch
import whisper
from pydub import AudioSegment
from pydub.silence import detect_nonsilent
import json
import pprint
from transformers import MarianMTModel, MarianTokenizer,pipeline 
import math
import argparse


# ---- C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n ----
# L·∫•y ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi c·ªßa t·ªáp script n√†y (B√äN TRONG container)
# v√≠ d·ª•: /app/main.py
script_path = os.path.abspath(__file__)

# L·∫•y ƒë∆∞·ªùng d·∫´n th∆∞ m·ª•c cha ch·ª©a t·ªáp script
# BASE_DIR s·∫Ω l√†: /app
BASE_DIR = os.path.dirname(script_path)

# X√¢y d·ª±ng c√°c ƒë∆∞·ªùng d·∫´n kh√°c d·ª±a tr√™n BASE_DIR
SOURCE_FOLDER = os.path.join(BASE_DIR, "source")
VIDEO_INPUT_NAME = "test1.mp4"
AUDIO_OUTPUT_NAME = "original_audio.wav" # B∆∞·ªõc 1 +2

VIDEO_INPUT_PATH = os.path.join(SOURCE_FOLDER, VIDEO_INPUT_NAME)
AUDIO_OUTPUT_PATH = os.path.join(SOURCE_FOLDER, AUDIO_OUTPUT_NAME)

# T·ªáp JSON ch·ª©a k·∫øt qu·∫£ phi√™n √¢m
TRANSCRIPT_OUTPUT_NAME = "original_transcript.json" # B∆∞·ªõc 3
TRANSCRIPT_OUTPUT_PATH = os.path.join(SOURCE_FOLDER, TRANSCRIPT_OUTPUT_NAME)
# T·ªáp JSON d·ªãch thu·∫≠t Anh -> Vi·ªát
TRANSLATED_TRANSCRIPT_NAME = "translated_transcript.json" # B∆∞·ªõc 4
TRANSLATED_TRANSCRIPT_PATH = os.path.join(SOURCE_FOLDER, TRANSLATED_TRANSCRIPT_NAME)
# T·ªáp ch·ª©a m·∫£ng d·ªØ li·ªáu TTS
# Ch√∫ng ta c√≥ th·ªÉ l∆∞u n√≥ d∆∞·ªõi d·∫°ng t·ªáp .py ƒë·ªÉ d·ªÖ import sau n√†y
TTS_DATA_NAME = "tts_data.py" 
TTS_DATA_PATH = os.path.join(SOURCE_FOLDER, TTS_DATA_NAME)
# T·ªáp ƒë·∫ßu ra cho B∆∞·ªõc 6
FINAL_AUDIO_NAME = "dubbed_audio.wav"
FINAL_AUDIO_PATH = os.path.join(SOURCE_FOLDER, FINAL_AUDIO_NAME)
FINAL_VIDEO_NAME = "final_dubbed_video.mp4"
FINAL_VIDEO_PATH = os.path.join(SOURCE_FOLDER, FINAL_VIDEO_NAME)

# C·∫•u h√¨nh m√¥ h√¨nh
WHISPER_MODEL_NAME = "large-v3"
# M√¥ h√¨nh d·ªãch thu·∫≠t
TRANSLATION_MODEL_NAME = "Helsinki-NLP/opus-mt-en-vi"
# TRANSLATION_MODEL_NAME = "vinai/vinai-translate-en2vi"

# C·∫•u h√¨nh t√πy ch·ªçn cho Whisper
# ƒê√¢y l√† n∆°i b·∫°n "tinh ch·ªânh" (tune) ƒë·ªÉ s·ª≠a l·ªói m·ªëc th·ªùi gian
WHISPER_OPTIONS = {
    "no_speech_threshold": 0.3,  # H·∫° th·∫•p ng∆∞·ª°ng ƒë·ªÉ d·ªÖ ph√°t hi·ªán im l·∫∑ng h∆°n (M·∫∑c ƒë·ªãnh 0.6)
    "hallucination_silence_threshold": 3.0, # X√≥a ·∫£o gi√°c trong kho·∫£ng l·∫∑ng > 3 gi√¢y
    "word_timestamps": True,     # B·∫≠t ƒë·ªÉ tƒÉng ƒë·ªô ch√≠nh x√°c c·ªßa m·ªëc th·ªùi gian
    "fp16": False                # ƒê·∫∑t l√† False n·∫øu ch·∫°y tr√™n CPU (an to√†n)
}
# C·∫•u h√¨nh VAD (Voice Activity Detection)
VAD_OPTIONS = {
    "min_silence_len": 1000, # (ms) Kho·∫£ng l·∫∑ng t·ªëi thi·ªÉu ƒë·ªÉ t√≠nh l√† "im l·∫∑ng"
    "silence_thresh": -2,   # Gi√° tr·ªã cao h∆°n: Ch·ªâ nh·ªØng √¢m thanh th·ª±c s·ª± l·ªõn m·ªõi ƒë∆∞·ª£c coi l√† "c√≥ ti·∫øng".
    "keep_silence": 250      # (ms) Gi·ªØ l·∫°i m·ªôt ch√∫t im l·∫∑ng ·ªü ƒë·∫ßu/cu·ªëi
}
# ----------------------------------------

def get_device() -> str:
    """Ki·ªÉm tra v√† tr·∫£ v·ªÅ thi·∫øt b·ªã (device) ph√π h·ª£p cho PyTorch."""
    if torch.cuda.is_available():
        print("Ph√°t hi·ªán GPU CUDA. ƒêang s·ª≠ d·ª•ng 'cuda'.")
        return "cuda"
    elif torch.backends.mps.is_available():
        print("Ph√°t hi·ªán Apple Silicon (M-series). ƒêang s·ª≠ d·ª•ng 'mps'.")
        return "mps"
    else:
        print("Kh√¥ng ph√°t hi·ªán GPU/MPS. ƒêang s·ª≠ d·ª•ng 'cpu'.")
        return "cpu"
    


def transcribe_audio(audio_path: str, model_name: str, device: str) -> list[dict] | None:
    """
    Phi√™n √¢m b·∫±ng VAD + Whisper ƒë·ªÉ c√≥ m·ªëc th·ªùi gian ch√≠nh x√°c.
    """
    # print(f"\nB·∫Øt ƒë·∫ßu B∆∞·ªõc 3: Phi√™n √¢m (S·ª≠ d·ª•ng VAD)...")
    try:
        # 1. T·∫£i m√¥ h√¨nh Whisper
        # print(f"ƒêang t·∫£i m√¥ h√¨nh Whisper '{model_name}'...")
        model = whisper.load_model(model_name, device=device)
        # print("T·∫£i m√¥ h√¨nh ho√†n t·∫•t.")

        # C·∫≠p nh·∫≠t t√πy ch·ªçn fp16
        transcribe_options = WHISPER_OPTIONS.copy()
        transcribe_options["fp16"] = (device != "cpu")
        # print(f"ƒêang phi√™n √¢m v·ªõi c√°c t√πy ch·ªçn: {transcribe_options}")

        # 2. T·∫£i √¢m thanh b·∫±ng Pydub
        # print(f"ƒêang t·∫£i √¢m thanh t·ª´: {audio_path}")
        audio = AudioSegment.from_wav(audio_path)

        # 3. Ch·∫°y VAD (Ph√°t hi·ªán c√°c ƒëo·∫°n kh√¥ng im l·∫∑ng)
        # print(f"ƒêang ch·∫°y VAD (Ph√°t hi·ªán gi·ªçng n√≥i)...")
        speech_chunks = detect_nonsilent(
            audio,
            min_silence_len=VAD_OPTIONS["min_silence_len"],
            silence_thresh=VAD_OPTIONS["silence_thresh"]
        )
        
        if not speech_chunks:
            print("‚ùå L·ªñI: VAD kh√¥ng t√¨m th·∫•y b·∫•t k·ª≥ gi·ªçng n√≥i n√†o trong t·ªáp.")
            return None

        total_chunks = len(speech_chunks)
        print(f"VAD ƒë√£ t√¨m th·∫•y {total_chunks} ƒëo·∫°n c√≥ gi·ªçng n√≥i.")
        
        all_segments = []
        segment_id_counter = 0
        temp_chunk_path = os.path.join(SOURCE_FOLDER, "temp_chunk.wav") # ƒê·ªãnh nghƒ©a 1 l·∫ßn

        # 4. L·∫∑p qua t·ª´ng ƒëo·∫°n c√≥ ti·∫øng v√† ch·∫°y Whisper
        for i, chunk_ms in enumerate(speech_chunks):
            original_start_ms, original_end_ms = chunk_ms
            
            # === LOG M·ªöI ===
            print(f"\n   --- VAD Chunk {i+1}/{total_chunks} ---")
            print(f"   ƒêo·∫°n VAD g·ªëc: {original_start_ms/1000:.2f}s -> {original_end_ms/1000:.2f}s")
            
            # Gi·ªØ l·∫°i m·ªôt ch√∫t ƒë·ªám im l·∫∑ng (t√πy ch·ªçn)
            start_ms = max(0, original_start_ms - VAD_OPTIONS["keep_silence"])
            end_ms = min(len(audio), original_end_ms + VAD_OPTIONS["keep_silence"])
            
            # === LOG M·ªöI ===
            print(f"   ƒêo·∫°n ƒë√£ ƒë·ªám (g·ª≠i cho Whisper): {start_ms/1000:.2f}s -> {end_ms/1000:.2f}s (Th·ªùi l∆∞·ª£ng: {(end_ms-start_ms)/1000:.2f}s)")
            
            # C·∫Øt ƒëo·∫°n √¢m thanh
            audio_chunk = audio[start_ms:end_ms]
            
            # C·∫ßn l∆∞u ra t·ªáp t·∫°m ƒë·ªÉ Whisper ƒë·ªçc
            audio_chunk.export(temp_chunk_path, format="wav")

            # 5. Ch·∫°y Whisper tr√™n ƒëo·∫°n √¢m thanh ƒë√£ c·∫Øt
            # === LOG M·ªöI ===
            print(f"   ...ƒêang ch·∫°y Whisper tr√™n ƒëo·∫°n n√†y...")
            result = model.transcribe(temp_chunk_path, task="transcribe", **transcribe_options)
            
            if not result['segments']:
                # === LOG M·ªöI ===
                print(f"   ...Whisper kh√¥ng t√¨m th·∫•y vƒÉn b·∫£n n√†o trong ƒëo·∫°n n√†y.")
                continue # B·ªè qua n·∫øu Whisper kh√¥ng nghe th·∫•y g√¨

            # === LOG M·ªöI ===
            print(f"   ...Whisper t√¨m th·∫•y {len(result['segments'])} segment(s) trong ƒëo·∫°n n√†y:")

            # 6. ƒêi·ªÅu ch·ªânh l·∫°i m·ªëc th·ªùi gian
            for segment in result['segments']:
                # T√≠nh to√°n m·ªëc th·ªùi gian cu·ªëi c√πng b·∫±ng c√°ch c·ªông offset
                offset_start_sec = (segment['start'] * 1000 + start_ms) / 1000.0
                offset_end_sec = (segment['end'] * 1000 + start_ms) / 1000.0
                
                new_segment = {
                    'id': segment_id_counter,
                    'start': offset_start_sec,
                    'end': offset_end_sec,
                    'text': segment['text']
                }
                
                # === LOG M·ªöI ===
                text_preview = segment['text'].strip()[:50] # L·∫•y 50 k√Ω t·ª± ƒë·∫ßu
                if len(segment['text'].strip()) > 50:
                    text_preview += "..."
                print(f"      -> Segment ID {segment_id_counter}: [{offset_start_sec:.2f}s -> {offset_end_sec:.2f}s] {text_preview}")

                all_segments.append(new_segment)
                segment_id_counter += 1
        
        # D·ªçn d·∫πp t·ªáp t·∫°m
        if os.path.exists(temp_chunk_path):
            os.remove(temp_chunk_path)

        print(f"‚úÖ B∆∞·ªõc 3 ho√†n th√†nh! ƒê√£ phi√™n √¢m {len(all_segments)} segments.")
        
        if all_segments:
            seg0 = all_segments[0]
            print(f"   Ki·ªÉm tra: Segment 0 (ID {seg0['id']}) b·∫Øt ƒë·∫ßu t·ª´ {seg0['start']:.2f}s")

        return all_segments

    except Exception as e:
        print(f"‚ùå L·ªñI trong qu√° tr√¨nh phi√™n √¢m VAD: {e}")
        return None
        
def translate_segments(whisper_result: dict, model_name: str, device: str, batch_size: int = 8 ) -> dict | None:
    """
    D·ªãch c√°c segment vƒÉn b·∫£n t·ª´ Anh sang Vi·ªát, gi·ªØ nguy√™n c·∫•u tr√∫c dict.
    T·ªëi ∆∞u h√≥a ƒë·ªÉ s·ª≠ d·ª•ng b·ªô nh·ªõ ·ªïn ƒë·ªãnh (KH·∫ÆC PH·ª§C R√í R·ªà B·ªò NH·ªö MPS/CUDA).

    Args:
        whisper_result (dict): json phi√™n √¢m (text englist)
        model_name (str): Model d·ªãch thu·∫≠t (Helsinki-NLP/opus-mt-en-vi, ...)
        device (str): cuda, mps, cpu

    Returns:
        dict | None: json 
    """
    try:
        # 1. T·∫£i m√¥ h√¨nh v√† tokenizer
        tokenizer = MarianTokenizer.from_pretrained(model_name)
        model = MarianMTModel.from_pretrained(model_name)
        model.to(device)

        # 2. T·∫°o b·∫£n sao d·ªØ li·ªáu NGAY T·ª™ ƒê·∫¶U
        segments_to_translate = whisper_result.get('segments', [])
        if not segments_to_translate:
            print("‚ùå L·ªñI: Kh√¥ng t√¨m th·∫•y 'segments' trong d·ªØ li·ªáu ƒë·∫ßu v√†o.")
            return None
            
        translated_data = json.loads(json.dumps(whisper_result))
        
        # Ch·ªâ l·∫•y ƒëo·∫°n text ti·∫øng anh
        texts_to_translate = [seg['text'].strip() for seg in segments_to_translate]
        total_segments = len(texts_to_translate)

        # 3. Logic batching
        total_batches = math.ceil(total_segments / batch_size)
        
        print(f"Chu·∫©n b·ªã d·ªãch {total_segments} segments th√†nh {total_batches} batches ({batch_size} segments/batch)...")

        for i in range(0, total_segments, batch_size):
            batch_texts = texts_to_translate[i : i + batch_size]
            
            # --- B·∫ÆT ƒê·∫¶U KH·ªêI T·ªêN B·ªò NH·ªö ---
            batch_tokens = tokenizer(
                batch_texts, 
                return_tensors="pt", 
                padding=True
            ).to(device)
            
            translated_tokens = model.generate(**batch_tokens)
            
            translated_batch_texts = [
                tokenizer.decode(t, skip_special_tokens=True) for t in translated_tokens
            ]
            # --- K·∫æT TH√öC KH·ªêI T·ªêN B·ªò NH·ªö ---

            # C·∫≠p nh·∫≠t tr·ª±c ti·∫øp v√†o 'translated_data'
            for j in range(len(translated_batch_texts)):
                segment_index = i + j
                translated_data['segments'][segment_index]['original_text'] = translated_data['segments'][segment_index]['text']
                translated_data['segments'][segment_index]['text'] = translated_batch_texts[j]

            # IN LOG TI·∫æN ƒê·ªò
            current_batch_num = (i // batch_size) + 1
            segments_done = min(i + batch_size, total_segments)
            print(f"   ... ƒê√£ d·ªãch xong batch {current_batch_num} / {total_batches} (Ho√†n th√†nh {segments_done}/{total_segments} segments)")

            # ==========================================================
            # === D·ªåN D·∫∏P B·ªò NH·ªö TH·ª¶ C√îNG (R·∫§T QUAN TR·ªåNG) ===
            del batch_tokens
            del translated_tokens
            if device == "mps":
                torch.mps.empty_cache()
            elif device == "cuda":
                torch.cuda.empty_cache()
            # ==========================================================

        print("D·ªãch thu·∫≠t ho√†n t·∫•t.")

        # 4. Tr·∫£ v·ªÅ k·∫øt qu·∫£
        print(f"‚úÖ B∆∞·ªõc 4 ho√†n th√†nh!")
        return translated_data
        
    except Exception as e:
        print(f"‚ùå L·ªñI trong qu√° tr√¨nh d·ªãch thu·∫≠t: {e}")
        import traceback
        traceback.print_exc()
        return None
    
def generate_tts_data_file(translated_segments: list[dict], output_script_path: str):
    """
    T·∫°o m·∫£ng d·ªØ li·ªáu TTS v√† GHI N·ªòI DUNG M·∫¢NG ƒë√≥ ra t·ªáp.
    ƒê·ªìng th·ªùi tr·∫£ v·ªÅ danh s√°ch segments ƒë√£ c·∫≠p nh·∫≠t cho B∆∞·ªõc 6.
    """
    # print(f"\nB·∫Øt ƒë·∫ßu B∆∞·ªõc 5: Ghi m·∫£ng d·ªØ li·ªáu TTS v√†o '{output_script_path}'...")
    
    tts_data_list = []
    segments_with_audio_path = []
    
    try:
        # L·∫∑p qua c√°c segment ƒë√£ d·ªãch
        for segment in translated_segments['segments']:
            segment_id = segment['id']
            text_to_speak = segment['text'].strip()
            
            # 1. T·∫°o d·ªØ li·ªáu cho m·∫£ng
            formatted_text = f"[KienThucQuanSu]{text_to_speak}"
            audio_output_file = f"audio_VN/{segment_id}.wav"
            
            tts_tuple = (formatted_text, audio_output_file)
            tts_data_list.append(tts_tuple)
            
            # 2. C·∫≠p nh·∫≠t segment cho B∆∞·ªõc 6
            segment['audio_path'] = os.path.join(SOURCE_FOLDER, audio_output_file)
            segments_with_audio_path.append(segment)

        # 3. Ghi m·∫£ng (d∆∞·ªõi d·∫°ng chu·ªói) ra t·ªáp
        with open(output_script_path, 'w', encoding='utf-8') as f:
            # S·ª≠ d·ª•ng pprint.pformat ƒë·ªÉ t·∫°o chu·ªói Python ƒë·∫πp
            # indent=4 v√† width=120 (ƒë·ªÉ tr√°nh ng·∫Øt d√≤ng qu√° s·ªõm)
            # S·∫Ω t·∫°o ra ƒë·ªãnh d·∫°ng gi·ªëng h·ªát v√≠ d·ª• c·ªßa b·∫°n
            file_content = pprint.pformat(tts_data_list, indent=4, width=120)
            
            # Ghi v√†o t·ªáp. 
            # (B·∫°n c√≥ th·ªÉ th√™m `tts_data = ` ·ªü ƒë·∫ßu n·∫øu mu·ªën n√≥ l√† t·ªáp .py)
            # f.write("tts_data = ")
            f.write(file_content)
            # f.write("\n") 

        print(f"‚úÖ B∆∞·ªõc 5 ho√†n th√†nh! ƒê√£ ghi m·∫£ng d·ªØ li·ªáu v√†o t·ªáp.")
        # Tr·∫£ v·ªÅ danh s√°ch segment ƒë√£ c·∫≠p nh·∫≠t cho B∆∞·ªõc 6
        return segments_with_audio_path

    except Exception as e:
        print(f"‚ùå L·ªñI trong qu√° tr√¨nh ghi t·ªáp d·ªØ li·ªáu TTS: {e}")
        return None
    
def apply_ffmpeg_atempo(input_segment: AudioSegment, speed: float, 
                        temp_dir: str = "/tmp") -> AudioSegment:
    """
    S·ª≠ d·ª•ng ffmpeg v·ªõi b·ªô l·ªçc 'atempo' ƒë·ªÉ co/d√£n √¢m thanh m·ªôt c√°ch an to√†n.
    H√†m n√†y x·ª≠ l√Ω c√°c gi·ªõi h·∫°n 0.5-100.0 c·ªßa atempo.
    """
    if abs(speed - 1.0) < 0.01:
        return input_segment # Kh√¥ng c·∫ßn thay ƒë·ªïi

    # T·∫°o ƒë∆∞·ªùng d·∫´n t·ªáp t·∫°m
    # Ch√∫ng ta ph·∫£i l∆∞u segment ra t·ªáp ƒë·ªÉ ffmpeg ƒë·ªçc
    temp_input = os.path.join(temp_dir, "temp_atempo_in.wav")
    temp_output = os.path.join(temp_dir, "temp_atempo_out.wav")
    
    input_segment.export(temp_input, format="wav")

    # X√¢y d·ª±ng chu·ªói b·ªô l·ªçc atempo
    # V√≠ d·ª•: speed = 0.3 -> [0.6, 0.5] (v√¨ 0.5 * 0.6 = 0.3)
    # V√≠ d·ª•: speed = 0.2 -> [0.8, 0.5, 0.5] (v√¨ 0.5 * 0.5 * 0.8 = 0.2)
    filters = []
    current_speed = speed
    
    # X·ª≠ l√Ω t·ªëc ƒë·ªô qu√° th·∫•p (< 0.5)
    while current_speed < 0.5:
        filters.append("atempo=0.5")
        current_speed /= 0.5 # T·ªëc ƒë·ªô c√≤n l·∫°i ƒë·ªÉ √°p d·ª•ng
    
    # X·ª≠ l√Ω t·ªëc ƒë·ªô qu√° cao (> 100.0)
    while current_speed > 100.0:
        filters.append("atempo=100.0")
        current_speed /= 100.0

    # √Åp d·ª•ng ph·∫ßn t·ªëc ƒë·ªô c√≤n l·∫°i (v√≠ d·ª•: 0.6, ho·∫∑c 1.5, ho·∫∑c 0.8)
    if abs(current_speed - 1.0) > 0.01:
        filters.append(f"atempo={current_speed}")

    # N·ªëi c√°c b·ªô l·ªçc l·∫°i, v√≠ d·ª•: "atempo=0.8,atempo=0.5,atempo=0.5"
    filter_chain = ",".join(filters)

    # X√¢y d·ª±ng v√† ch·∫°y l·ªánh ffmpeg
    command = [
        'ffmpeg',
        '-i', temp_input,
        '-filter:a', filter_chain,
        '-y', temp_output
    ]
    
    try:
        subprocess.run(command, check=True, capture_output=True, text=True)
        # T·∫£i t·ªáp k·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c co/d√£n
        output_segment = AudioSegment.from_wav(temp_output)
        
        # D·ªçn d·∫πp t·ªáp t·∫°m
        os.remove(temp_input)
        os.remove(temp_output)
        
        return output_segment
        
    except Exception as e:
        print(f"   ‚ùå L·ªñI khi ƒëang ch·∫°y atempo (t·ªëc ƒë·ªô {speed:.2f}x): {e}")
        print(f"   ...S·ª≠ d·ª•ng segment g·ªëc (kh√¥ng ƒë·ªìng b·ªô) thay th·∫ø.")
        # D·ªçn d·∫πp t·ªáp t·∫°m
        if os.path.exists(temp_input): os.remove(temp_input)
        if os.path.exists(temp_output): os.remove(temp_output)
        return input_segment # Tr·∫£ v·ªÅ b·∫£n g·ªëc n·∫øu th·∫•t b·∫°i
    
    
def synchronize_and_combine(segments_with_audio_path: list[dict], 
                            final_audio_path: str) -> str | None:
    """
    (B∆∞·ªõc 6.1) ƒê·ªìng b·ªô (co/d√£n) c√°c t·ªáp TTS v√† n·ªëi ch√∫ng l·∫°i.
    Phi√™n b·∫£n n√†y s·ª≠ d·ª•ng ffmpeg atempo thay v√¨ pydub.speedup.
    """
    print(f"\nB·∫Øt ƒë·∫ßu B∆∞·ªõc 6.1: ƒê·ªìng b·ªô v√† N·ªëi c√°c t·ªáp √¢m thanh...")
    
    final_audio = AudioSegment.empty()
    last_segment_end_ms = 0.0 # Theo d√µi m·ªëc th·ªùi gian cu·ªëi c√πng (t√≠nh b·∫±ng ms)
    
    try:
        # L·∫∑p qua c√°c segment ƒë√£ c√≥ ƒë∆∞·ªùng d·∫´n 'audio_path'
        for i, segment in enumerate(segments_with_audio_path):
            
            print(f"--- ƒêang x·ª≠ l√Ω segment {i} (ID: {segment['id']}) ---")
            
            target_start_ms = segment['start'] * 1000
            target_end_ms = segment['end'] * 1000
            target_duration_ms = target_end_ms - target_start_ms

            # 1. X·ª≠ l√Ω kho·∫£ng l·∫∑ng (Silence)
            if target_start_ms > last_segment_end_ms:
                silence_duration = target_start_ms - last_segment_end_ms
                final_audio += AudioSegment.silent(duration=silence_duration)
                print(f"   ... Th√™m {silence_duration:.0f}ms kho·∫£ng l·∫∑ng.")
                
            # 2. T·∫£i t·ªáp √¢m thanh TTS
            audio_file_path = segment['audio_path']
            if not os.path.exists(audio_file_path):
                print(f"   ‚ö†Ô∏è C·∫¢NH B√ÅO: Kh√¥ng t√¨m th·∫•y t·ªáp {audio_file_path}. B·ªè qua segment.")
                last_segment_end_ms = target_end_ms
                continue

            tts_segment = AudioSegment.from_wav(audio_file_path)
            current_duration_ms = len(tts_segment)
            
            # 3. ƒê·ªìng b·ªô th·ªùi gian (Time-Stretching)
            if target_duration_ms <= 0 or current_duration_ms <= 0:
                print(f"   ‚ö†Ô∏è C·∫¢NH B√ÅO: Segment {i} c√≥ th·ªùi l∆∞·ª£ng kh√¥ng h·ª£p l·ªá. B·ªè qua.")
                last_segment_end_ms = target_end_ms
                continue
            
            playback_speed = current_duration_ms / target_duration_ms

            print(f"   ƒê·ªìng b·ªô segment {i}: {current_duration_ms:.0f}ms -> {target_duration_ms:.0f}ms (t·ªëc ƒë·ªô {playback_speed:.2f}x)")

            # === KH·ªêI LOGIC M·ªöI (V3.0) ===
            # G·ªçi h√†m helper ffmpeg atempo c·ªßa ch√∫ng ta
            processed_segment = apply_ffmpeg_atempo(tts_segment, playback_speed)
            # === K·∫æT TH√öC KH·ªêI LOGIC M·ªöI ===

            # 4. N·ªëi √¢m thanh ƒë√£ x·ª≠ l√Ω
            final_audio += processed_segment
            last_segment_end_ms = target_end_ms

        # 5. L∆∞u t·ªáp √¢m thanh cu·ªëi c√πng
        print(f"ƒêang l∆∞u t·ªáp √¢m thanh l·ªìng ti·∫øng cu·ªëi c√πng t·∫°i: {final_audio_path}")
        final_audio.export(final_audio_path, format="wav")
        print(f"‚úÖ B∆∞·ªõc 6.1 ho√†n th√†nh!")
        return final_audio_path

    except Exception as e:
        print(f"‚ùå L·ªñI trong qu√° tr√¨nh ƒë·ªìng b·ªô √¢m thanh: {e}")
        return None
def merge_short_segments(segments: list[dict], max_gap_sec: float = 1.5, min_segment_len_sec: float = 2.0) -> list[dict]:
    """
    H·ª£p nh·∫•t c√°c segment ng·∫Øn d·ª±a tr√™n kho·∫£ng l·∫∑ng v√† ƒë·ªô d√†i.
    """
    if not segments:
        return []

    print(f"\nB·∫Øt ƒë·∫ßu H·ª£p nh·∫•t: c√≥ {len(segments)} segments ban ƒë·∫ßu.")
    
    merged_segments = []
    
    # B·∫Øt ƒë·∫ßu v·ªõi segment ƒë·∫ßu ti√™n
    current_segment = segments[0].copy() 
    
    for i in range(1, len(segments)):
        next_segment = segments[i]
        
        # T√≠nh kho·∫£ng l·∫∑ng gi·ªØa 2 segment
        gap = next_segment['start'] - current_segment['end']
        
        # T√≠nh th·ªùi l∆∞·ª£ng c·ªßa segment hi·ªán t·∫°i
        current_duration = current_segment['end'] - current_segment['start']
        
        # Ki·ªÉm tra ƒëi·ªÅu ki·ªán ƒë·ªÉ g·ªôp
        # 1. Kho·∫£ng l·∫∑ng gi·ªØa ch√∫ng ƒë·ªß nh·ªè (v√≠ d·ª•: < 1.5s)
        # 2. V√Ä segment hi·ªán t·∫°i qu√° ng·∫Øn (v√≠ d·ª•: < 2s)
        if gap <= max_gap_sec and current_duration <= min_segment_len_sec:
            # G·ªôp!
            # N·ªëi vƒÉn b·∫£n
            current_segment['text'] += " " + next_segment['text']
            # C·∫≠p nh·∫≠t th·ªùi gian k·∫øt th√∫c
            current_segment['end'] = next_segment['end']
            print(f"   -> ƒê√£ g·ªôp ID {current_segment['id']} v√† {next_segment['id']}")
        else:
            # Kh√¥ng g·ªôp, l∆∞u segment hi·ªán t·∫°i
            merged_segments.append(current_segment)
            # B·∫Øt ƒë·∫ßu segment m·ªõi
            current_segment = next_segment.copy()
            
    # ƒê·ª´ng qu√™n l∆∞u segment cu·ªëi c√πng!
    merged_segments.append(current_segment)
    
    print(f"‚úÖ H·ª£p nh·∫•t ho√†n t·∫•t: c√≤n {len(merged_segments)} segments.")
    return merged_segments
    
def merge_audio_to_video(video_input_path: str, audio_input_path: str, 
                         video_output_path: str) -> str | None:
    """
    Gh√©p t·ªáp √¢m thanh l·ªìng ti·∫øng v√†o video g·ªëc (ƒë√£ x√≥a ti·∫øng).
    """
    
    # L·ªánh ffmpeg
    # -i [video_input]: Video g·ªëc
    # -i [audio_input]: √Çm thanh l·ªìng ti·∫øng m·ªõi
    # -c:v copy: Sao ch√©p lu·ªìng video, kh√¥ng encode l·∫°i (R·∫§T NHANH)
    # -map 0:v:0: Ch·ªçn lu·ªìng video t·ª´ file ƒë·∫ßu v√†o (0)
    # -map 1:a:0: Ch·ªçn lu·ªìng audio t·ª´ file th·ª© hai (1) -> B·ªé √ÇM THANH G·ªêC
    # -shortest: K·∫øt th√∫c video khi lu·ªìng ng·∫Øn nh·∫•t (video ho·∫∑c audio) k·∫øt th√∫c
    # -y: Ghi ƒë√® file ƒë·∫ßu ra
    command = [
        'ffmpeg',
        '-i', video_input_path,
        '-i', audio_input_path,
        '-c:v', 'copy',
        '-map', '0:v:0',
        '-map', '1:a:0',
        '-shortest',
        '-y',
        video_output_path
    ]

    try:
        subprocess.run(command, check=True, capture_output=True, text=True)
        print(f"‚úÖ B∆∞·ªõc 6.2 ho√†n th√†nh! Video l·ªìng ti·∫øng ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i:")
        print(f"   {video_output_path}")
        return video_output_path
    except subprocess.CalledProcessError as e:
        print(f"‚ùå L·ªñI: ffmpeg th·∫•t b·∫°i khi gh√©p video: {e.stderr}")
        return None
    except Exception as e:
        print(f"‚ùå L·ªñI kh√¥ng x√°c ƒë·ªãnh khi gh√©p video: {e}")
        return None
    



def main():
    device = get_device()

    # ========== CH·∫æ ƒê·ªò 1: CHU·∫®N B·ªä (PREP) ==========  
    # --- B∆∞·ªõc 3: Phi√™n √¢m ---
    if os.path.exists(TRANSCRIPT_OUTPUT_PATH):
        with open(TRANSCRIPT_OUTPUT_PATH, 'r', encoding='utf-8') as f:
            segments = json.load(f)

    # --- B∆∞·ªõc 4: D·ªãch thu·∫≠t ---
    if os.path.exists(TRANSLATED_TRANSCRIPT_PATH):
        print(f"\nƒê√£ t√¨m th·∫•y t·ªáp d·ªãch thu·∫≠t: {TRANSLATED_TRANSCRIPT_PATH}. B·ªè qua B∆∞·ªõc 4.")
        with open(TRANSLATED_TRANSCRIPT_PATH, 'r', encoding='utf-8') as f:
            translated_data = json.load(f) # Load d·ªØ li·ªáu ƒë√£ d·ªãch
    else:
        # Truy·ªÅn TO√ÄN B·ªò dict (segments) v√†o
        translated_data = translate_segments(segments, TRANSLATION_MODEL_NAME, device)
        
        if translated_data is None:
            print("D·ªãch thu·∫≠t th·∫•t b·∫°i.")
            return
            
        # L∆∞u T·ªÜP DICT ƒê√É D·ªäCH
        try:
            with open(TRANSLATED_TRANSCRIPT_PATH, 'w', encoding='utf-8') as f:
                json.dump(translated_data, f, indent=4, ensure_ascii=False)
            print(f"‚úÖ ƒê√£ l∆∞u d·ªãch thu·∫≠t v√†o: {TRANSLATED_TRANSCRIPT_PATH}")
        except Exception as e:
            print(f"‚ùå L·ªñI: Kh√¥ng th·ªÉ l∆∞u t·ªáp JSON d·ªãch thu·∫≠t: {e}")
            sys.exit(1)

    # --- B∆∞·ªõc 5: Ghi t·ªáp d·ªØ li·ªáu TTS ---
    print("\nB·∫Øt ƒë·∫ßu B∆∞·ªõc 5: Ghi t·ªáp d·ªØ li·ªáu...")
    segments_with_audio_path = generate_tts_data_file(translated_data, TTS_DATA_PATH)
    if segments_with_audio_path is None: sys.exit(1)

    # C·∫≠p nh·∫≠t l·∫°i t·ªáp JSON v·ªõi ƒë∆∞·ªùng d·∫´n √¢m thanh
    try:
        with open(TRANSLATED_TRANSCRIPT_PATH, 'w', encoding='utf-8') as f:
            json.dump(segments_with_audio_path, f, indent=4, ensure_ascii=False)
        print(f"‚úÖ ƒê√£ c·∫≠p nh·∫≠t t·ªáp d·ªãch thu·∫≠t v·ªõi ƒë∆∞·ªùng d·∫´n √¢m thanh (d·ª± ki·∫øn).")
    except Exception as e:
        print(f"‚ùå L·ªñI: Kh√¥ng th·ªÉ c·∫≠p nh·∫≠t t·ªáp JSON d·ªãch thu·∫≠t: {e}")

    print("\n--- ‚úÖ Ho√†n th√†nh 'PREP' ---")
    print(f"ƒê√£ t·∫°o t·ªáp d·ªØ li·ªáu TTS t·∫°i: {TTS_DATA_PATH}")
    print("B√¢y gi·ªù b·∫°n c√≥ th·ªÉ t·∫°o c√°c t·ªáp .wav trong 'source/audio_VN' tr∆∞·ªõc khi ch·∫°y b∆∞·ªõc 'combine'.")

    # ========== CH·∫æ ƒê·ªò 2: K·∫æT H·ª¢P (COMBINE) ==========
    print("--- Ch·∫°y ch·∫ø ƒë·ªô 'COMBINE' (B∆∞·ªõc 6) ---")
    
    # --- B∆∞·ªõc 6: ƒê·ªìng b·ªô v√† Gh√©p ---
    # T·∫£i t·ªáp JSON ƒë√£ d·ªãch (ph·∫£i ch·ª©a 'audio_path')
    if not os.path.exists(TRANSLATED_TRANSCRIPT_PATH):
        print(f"‚ùå L·ªñI: Kh√¥ng t√¨m th·∫•y t·ªáp {TRANSLATED_TRANSCRIPT_PATH}.")
        print("B·∫°n ph·∫£i ch·∫°y b∆∞·ªõc 'prep' tr∆∞·ªõc.")
        sys.exit(1)
        
    print(f"ƒêang t·∫£i t·ªáp d·ªãch thu·∫≠t: {TRANSLATED_TRANSCRIPT_PATH}...")
    with open(TRANSLATED_TRANSCRIPT_PATH, 'r', encoding='utf-8') as f:
        translated_segments = json.load(f)

    # Ki·ªÉm tra xem c√°c t·ªáp audio c√≥ th·ª±c s·ª± t·ªìn t·∫°i kh√¥ng
    first_audio_path = translated_segments[0].get('audio_path')
    if first_audio_path is None or not os.path.exists(first_audio_path):
            print(f"‚ùå L·ªñI: Kh√¥ng t√¨m th·∫•y t·ªáp √¢m thanh ƒë·∫ßu ti√™n ({first_audio_path}).")
            print("B·∫°n ƒë√£ t·∫°o c√°c t·ªáp .wav trong 'source/audio_VN' ch∆∞a?")
            sys.exit(1)

    # B∆∞·ªõc 6.1: ƒê·ªìng b·ªô v√† N·ªëi √¢m thanh
    final_audio_file = synchronize_and_combine(translated_segments, FINAL_AUDIO_PATH)
    if final_audio_file is None:
        print("D·ª´ng ch∆∞∆°ng tr√¨nh do l·ªói ·ªü B∆∞·ªõc 6.1.")
        sys.exit(1)

    # B∆∞·ªõc 6.2: Gh√©p √¢m thanh v√†o video
    final_video_file = merge_audio_to_video(VIDEO_INPUT_PATH, final_audio_file, FINAL_VIDEO_PATH)
    if final_video_file is None:
        print("D·ª´ng ch∆∞∆°ng tr√¨nh do l·ªói ·ªü B∆∞·ªõc 6.2.")
        sys.exit(1)

    print("\n--- üéâüéâüéâ HO√ÄN TH√ÄNH TO√ÄN B·ªò D·ª∞ √ÅN! üéâüéâüéâ ---")
    print(f"Video l·ªìng ti·∫øng cu·ªëi c√πng c·ªßa b·∫°n ƒë√£ s·∫µn s√†ng t·∫°i:")
    print(f"{FINAL_VIDEO_PATH}")
    print("-------------------------------------------------")






main()